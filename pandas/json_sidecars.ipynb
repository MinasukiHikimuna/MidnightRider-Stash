{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stash app client\n",
    "\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "from libraries.client_stashapp import get_stashapp_client\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "stash = get_stashapp_client(\"AURAL_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Directory containing JSON files\n",
    "json_dir = os.getenv(\"JSON_SIDECARS_PATH\")\n",
    "\n",
    "# List to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "total_entries = 0\n",
    "\n",
    "# Loop through each JSON file in the directory\n",
    "for file_name in os.listdir(json_dir):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, file_name)\n",
    "\n",
    "        # Load the JSON file\n",
    "        imported_json = pd.read_json(file_path)\n",
    "\n",
    "        # Assuming the entries of interest are in a column named 'entries'\n",
    "        entries_column = imported_json['entries']\n",
    "        entries_df = pd.DataFrame(entries_column.tolist(), columns=['Post ID', 'Subreddit', 'Author', 'Content Type', 'Title', 'Timestamp', 'Upvotes', 'Length', 'Submitted By'])\n",
    "\n",
    "        # Convert the Unix timestamp to a formatted date\n",
    "        entries_df['Formatted Date'] = pd.to_datetime(entries_df['Timestamp'], unit='s').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Replace the 'Timestamp' column with 'Formatted Date'\n",
    "        entries_df['Timestamp'] = entries_df['Formatted Date']\n",
    "        entries_df = entries_df.drop(columns=['Formatted Date'])\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        entries_df = entries_df.drop(columns=['Length', 'Submitted By'])\n",
    "\n",
    "        # Append the processed DataFrame to the list\n",
    "        dataframes.append(entries_df)\n",
    "        total_entries += entries_df.shape[0]\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicates by 'Post ID'\n",
    "deduplicated_df = combined_df.drop_duplicates(subset=['Post ID'])\n",
    "\n",
    "print(f\"Deduplicated DataFrame shape: {deduplicated_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = stash.find_scenes({ 'title': { 'value': '', 'modifier': 'IS_NULL' }}, q = \"AprilW9\")\n",
    "df_scenes = pd.DataFrame(scenes)\n",
    "df_scenes['basename'] = df_scenes['files'].apply(lambda x: x[0]['basename'].replace('.mp4', '').replace('\\'', ''))\n",
    "df_scenes['author'] = df_scenes['basename'].apply(lambda x: x.split(' - ', 1)[0])\n",
    "df_scenes['reddit_title'] = df_scenes['basename'].apply(lambda x: x.split(' - ', 1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import fuzz\n",
    "\n",
    "# Function to find the best matching title and return relevant data\n",
    "def find_best_match(row, deduplicated_df):\n",
    "    # Filter deduplicated_df to only include rows with the same author\n",
    "    author_matches = deduplicated_df[deduplicated_df['Author'] == row['author']]\n",
    "    \n",
    "    # Calculate similarity for each title\n",
    "    if not author_matches.empty:\n",
    "        author_matches['similarity'] = author_matches['Title'].apply(lambda x: fuzz.token_set_ratio(row['reddit_title'], x))\n",
    "        \n",
    "        # Find the title with the highest similarity\n",
    "        best_match = author_matches.loc[author_matches['similarity'].idxmax()]\n",
    "        \n",
    "        # Return the relevant columns from df_scenes and deduplicated_df\n",
    "        return pd.Series([\n",
    "            row['id'], \n",
    "            row['author'], \n",
    "            row['reddit_title'],\n",
    "            best_match['Post ID'],\n",
    "            best_match['Subreddit'],\n",
    "            best_match['Author'],\n",
    "            best_match['Title'],\n",
    "        ])\n",
    "    else:\n",
    "        # If no matching author, return None values for deduplicated_df columns\n",
    "        return pd.Series([\n",
    "            row['id'], \n",
    "            row['author'], \n",
    "            row['reddit_title'],\n",
    "            None,  # Post ID\n",
    "            None,  # Subreddit\n",
    "            None,  # Author\n",
    "            None   # Title\n",
    "        ])\n",
    "\n",
    "# Apply the function to df_scenes\n",
    "matched_df = df_scenes.apply(lambda row: find_best_match(row, deduplicated_df), axis=1)\n",
    "\n",
    "# Assign column names to the new DataFrame\n",
    "matched_df.columns = ['id', 'author', 'reddit_title', 'Post ID', 'Subreddit', 'Author', 'Title']\n",
    "\n",
    "# Place Title columns next to each other\n",
    "matched_df = matched_df[['id', 'author', 'Post ID', 'reddit_title', 'Title', 'Subreddit', 'Author']]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(matched_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_matched_df = matched_df[~matched_df['id'].isin([\"120\", \"123\", \"148\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, scene in manually_matched_df.iterrows():\n",
    "    stash.update_scene({\n",
    "        'id': scene['id'],\n",
    "        'title': None,\n",
    "        'code': scene['Post ID'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all authors in df_scenes are found in deduplicated_df\n",
    "all_authors_in_deduplicated = df_scenes['author'].isin(deduplicated_df['Author'])\n",
    "\n",
    "# Check if there are any authors in df_scenes that are not in deduplicated_df\n",
    "missing_authors = df_scenes.loc[~all_authors_in_deduplicated, 'author']\n",
    "\n",
    "# Display results\n",
    "if missing_authors.empty:\n",
    "    print(\"All authors in df_scenes are found in deduplicated_df.\")\n",
    "else:\n",
    "    print(\"The following authors in df_scenes are not found in deduplicated_df:\")\n",
    "    print(missing_authors.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Assuming 'scenes' is your list of scene dictionaries\n",
    "filtered_scenes = []\n",
    "\n",
    "for scene in scenes:\n",
    "    # Check if the scene has exactly one file\n",
    "    if len(scene['files']) == 1:\n",
    "        file_info = scene['files'][0]\n",
    "        file_path = file_info['path']\n",
    "        \n",
    "        # Construct the expected JSON sidecar path\n",
    "        json_sidecar_path = os.path.splitext(file_path)[0] + '.json'\n",
    "        \n",
    "        # Check if the JSON sidecar file exists\n",
    "        if os.path.exists(json_sidecar_path):\n",
    "            # Load the JSON sidecar file\n",
    "            with open(json_sidecar_path, 'r') as json_file:\n",
    "                sidecar_data = json.load(json_file)\n",
    "                \n",
    "                # Extract the required fields from the JSON sidecar\n",
    "                urls = sidecar_data.get('urls', [])\n",
    "                cleaned_urls = [url.replace('old.reddit.com', 'www.reddit.com') for url in urls]\n",
    "\n",
    "                title = sidecar_data.get('title', '')\n",
    "                author = sidecar_data.get('author', '')\n",
    "                \n",
    "                # Add these details to the scene dictionary\n",
    "                scene['sidecar_urls'] = cleaned_urls\n",
    "                scene['sidecar_title'] = title\n",
    "                scene['sidecar_author'] = author\n",
    "                \n",
    "                # Append the scene to the filtered list\n",
    "                filtered_scenes.append(scene)\n",
    "\n",
    "# Convert the filtered scenes to a DataFrame\n",
    "df_filtered_scenes = pd.DataFrame(filtered_scenes)\n",
    "\n",
    "# Output the filtered scenes with sidecar information\n",
    "df_filtered_scenes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(client_id = os.getenv(\"REDDIT_CLIENT_ID\"), client_secret = os.getenv(\"REDDIT_CLIENT_SECRET\"), password = os.getenv(\"REDDIT_CLIENT_PASSWORD\"), user_agent = os.getenv(\"REDDIT_CLIENT_USER_AGENT\"), username = os.getenv(\"REDDIT_CLIENT_USERNAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, scene in df_filtered_scenes.iterrows():\n",
    "    stash_performers = stash.find_performers({ 'name': { 'value': scene['sidecar_author'], 'modifier': 'EQUALS' } })\n",
    "    if len(stash_performers) == 1:\n",
    "        stash_performer = stash_performers[0]\n",
    "\n",
    "        stash.update_scene({\n",
    "            'id': scene['id'],\n",
    "            'title': scene['sidecar_title'],\n",
    "            'performer_ids': [stash_performer['id']],\n",
    "            'urls': scene['sidecar_urls']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes_for_update = stash.find_scenes({ 'title': { 'value': '', 'modifier': 'IS_NULL' }, 'code': { 'value': '', 'modifier': 'NOT_NULL' } }, q = \"AprilW9\")\n",
    "df_scenes_for_update = pd.DataFrame(scenes_for_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "for index, scene in df_scenes_for_update.iterrows():\n",
    "    scene_for_scraping = stash.find_scene(scene['id'])\n",
    "\n",
    "    # reddit_url = next((url for url in scene_for_scraping['urls'] if 'reddit.com' in url), None)\n",
    "    # if not reddit_url:\n",
    "    #     raise Exception('No Reddit URL found')\n",
    "\n",
    "    submission = reddit.submission(id = scene_for_scraping['code'])\n",
    "\n",
    "    stash.update_scene({\n",
    "        'id': scene_for_scraping['id'],\n",
    "        'code': submission.id,\n",
    "        'title': submission.title,\n",
    "        'performer_ids': [stash_performer['id']],\n",
    "        'urls': scene_for_scraping['urls'],\n",
    "        'date': datetime.datetime.fromtimestamp(submission.created_utc, tz=datetime.UTC).strftime('%Y-%m-%d'),\n",
    "        'details': submission.selftext\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprilw9_gwasi = deduplicated_df[deduplicated_df['Author'] == 'AprilW9']\n",
    "aprilw9_gwasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprilw9_gwasi[aprilw9_gwasi['Title'].str.contains('Love Blowing')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprilw9_scenes = stash.find_scenes({ 'title': { 'value': '', 'modifier': 'IS_NULL' } }, q = \"AprilW9\")\n",
    "df_aprilw9_scenes = pd.DataFrame(aprilw9_scenes)\n",
    "\n",
    "df_aprilw9_scenes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
