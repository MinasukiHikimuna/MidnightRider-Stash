{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"F:\\\\Culture Data Scraping\\\\The Art Of Blowjob\\\\index.html\"\n",
    "html_content = open(path, 'r', encoding='utf-8').read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Assuming you have the HTML content in a variable called 'html_content'\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "data = []\n",
    "\n",
    "for div in soup.find_all('div', class_=['update-side', 'update-middle']):\n",
    "    link = div.find('a')['href']\n",
    "    thumbnail = div.find('img')['src']\n",
    "    title = div.find('h3').text.strip()\n",
    "    datetime = div.find('time')['datetime']\n",
    "    duration_text = div.find('p', class_='datetime').text.split('~')[-1].strip()\n",
    "    \n",
    "    # Convert duration to seconds\n",
    "    duration_parts = re.findall(r'\\d+', duration_text)\n",
    "    duration_seconds = sum(int(x) * 60 ** i for i, x in enumerate(reversed(duration_parts)))\n",
    "    \n",
    "    data.append({\n",
    "        'link': link,\n",
    "        'thumbnail': thumbnail,\n",
    "        'title': title,\n",
    "        'datetime': datetime,\n",
    "        'duration_seconds': duration_seconds,\n",
    "        'large_image': None,\n",
    "        'description': None,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Assuming 'df' is your existing DataFrame with 'link' and 'description' columns\n",
    "\n",
    "# Function to fetch and parse description\n",
    "def get_description(link):\n",
    "    full_link = urljoin('https://web.archive.org', link)\n",
    "    response = requests.get(full_link)\n",
    "    page_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article = page_soup.find('div', class_='article')\n",
    "    if article:\n",
    "        paragraphs = article.find_all('p')\n",
    "        description = ' '.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "    else:\n",
    "        description = ''\n",
    "    \n",
    "    return description\n",
    "\n",
    "# Iterate through rows without description\n",
    "for index, row in df[df['description'].isnull()].iterrows():\n",
    "    try:\n",
    "        description = get_description(row['link'])\n",
    "        df.at[index, 'description'] = description\n",
    "        print(f\"Added description for {row['title']}\")\n",
    "        \n",
    "        # Add a longer, randomized delay to avoid triggering blocks\n",
    "        delay = random.uniform(5, 10)\n",
    "        print(f\"Waiting for {delay:.2f} seconds before next request...\")\n",
    "        time.sleep(delay)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['title']}: {str(e)}\")\n",
    "        print(\"Waiting for 60 seconds before retrying...\")\n",
    "        time.sleep(60)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert thumbnail URL to large image URL\n",
    "def get_large_image_url(thumb_url):\n",
    "    if thumb_url and '-th.jpg' in thumb_url:\n",
    "        return thumb_url.replace('-th.jpg', '-lg.jpg').replace('20170606190201', '20170709133354')\n",
    "    return None\n",
    "\n",
    "# Apply the function to rows where large_image is not set\n",
    "mask = df['large_image'].isnull()\n",
    "df.loc[mask, 'large_image'] = df.loc[mask, 'thumbnail'].apply(get_large_image_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable, write_deltalake\n",
    "import pyarrow as pa\n",
    "\n",
    "# Convert Pandas DataFrame to PyArrow Table\n",
    "arrow_table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Save as Delta table to a local directory\n",
    "local_path = \"F:\\\\Culture Data Scraping\\\\The Art Of Blowjob\\\\delta_table\"\n",
    "\n",
    "# Write the data to the Delta table\n",
    "# Use mode=\"overwrite\" to replace all data, or mode=\"append\" to add new data\n",
    "write_deltalake(local_path, arrow_table, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean description\n",
    "def clean_description(desc):\n",
    "    if isinstance(desc, str):\n",
    "        # Remove \"Updated on <date> \" pattern\n",
    "        cleaned = re.sub(r'^Updated on [A-Za-z]+, [A-Za-z]+ \\d+, \\d+ ', '', desc)\n",
    "        return cleaned.strip()\n",
    "    return desc\n",
    "\n",
    "# Apply the cleaning function to the 'description' column\n",
    "df['clean_description'] = df['description'].apply(clean_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all versions of the Delta table\n",
    "delta_table = DeltaTable(local_path)\n",
    "history = delta_table.history()\n",
    "\n",
    "print(\"All versions of the Delta table:\")\n",
    "for row in history:\n",
    "    print(f\"Version: {row['version']}, Timestamp: {row['timestamp']}, Operation: {row['operation']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "def download_image(url, folder, filename):\n",
    "    file_path = os.path.join(folder, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File already exists: {filename}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(f\"{url}\", stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        \n",
    "        with open(file_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        \n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {filename}: {str(e)}\")\n",
    "\n",
    "def extract_date_from_url(url):\n",
    "    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    match = re.search(date_pattern, url)\n",
    "    return match.group(0) if match else 'unknown_date'\n",
    "\n",
    "# Get the directory of the HTML file\n",
    "html_dir = os.path.dirname(local_path)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    # Extract date from URL\n",
    "    date = extract_date_from_url(row['thumbnail'])\n",
    "    \n",
    "    # Download thumbnail\n",
    "    download_image(f\"https://web.archive.org{row['thumbnail']}\", os.path.join(html_dir, 'thumbnails'), f\"{date}_thumb.jpg\")\n",
    "    time.sleep(1)  # Add delay similar to reading descriptions\n",
    "    \n",
    "    # Download large image\n",
    "    download_image(f\"https://web.archive.org{row['large_image']}\", os.path.join(html_dir, 'large_images'), f\"{date}_large.jpg\")\n",
    "    time.sleep(1)  # Add delay similar to reading descriptions\n",
    "\n",
    "print(\"All images processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chloe Morgane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HTML file\n",
    "chloe_local_path = \"F:\\\\Culture Data Scraping\\\\Chloe Morgane\\\\updates.html\"\n",
    "with open(chloe_local_path, 'r', encoding='utf-8') as file:\n",
    "    chloe_html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "chloe_soup = BeautifulSoup(chloe_html_content, 'html.parser')\n",
    "\n",
    "# Find all update items\n",
    "chloe_update_items = chloe_soup.find_all('article', class_='updates-item')\n",
    "\n",
    "# Prepare lists to store data\n",
    "chloe_titles = []\n",
    "chloe_links = []\n",
    "chloe_thumbnail_urls = []\n",
    "chloe_categories = []\n",
    "chloe_dates = []  # New list to store dates\n",
    "\n",
    "# Extract information from each update item\n",
    "for item in chloe_update_items:\n",
    "    # Extract title\n",
    "    chloe_title = item.find('h3', class_='updates-name').text.strip()\n",
    "    chloe_titles.append(chloe_title)\n",
    "    \n",
    "    # Extract link\n",
    "    chloe_link = item.find('h3', class_='updates-name').find('a')['href']\n",
    "    chloe_links.append(chloe_link)\n",
    "    \n",
    "    # Extract thumbnail URL\n",
    "    chloe_thumbnail_url = item.find('img', class_='updates-poster')['src']\n",
    "    chloe_thumbnail_urls.append(chloe_thumbnail_url)\n",
    "    \n",
    "    # Determine category\n",
    "    chloe_category = 'photos' if '/photos/' in chloe_link else 'video'\n",
    "    chloe_categories.append(chloe_category)\n",
    "    \n",
    "    # Extract date from thumbnail URL\n",
    "    date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', chloe_thumbnail_url)\n",
    "    if date_match:\n",
    "        chloe_date = date_match.group(1)\n",
    "    else:\n",
    "        chloe_date = None  # or some default value if date is not found\n",
    "    chloe_dates.append(chloe_date)  # Append date to the list\n",
    "\n",
    "# Create a DataFrame\n",
    "chloe_df = pd.DataFrame({\n",
    "    'title': chloe_titles,\n",
    "    'link': chloe_links,\n",
    "    'thumbnail_url': chloe_thumbnail_urls,\n",
    "    'category': chloe_categories,\n",
    "    'date': chloe_dates  # Use the list of dates instead of a single value\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate video stream URLs for video releases\n",
    "chloe_df['video_stream_url'] = chloe_df.apply(\n",
    "    lambda row: row['link'].replace('/video/', '/video-stream/') if row['category'] == 'video' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get credentials from .env file\n",
    "username = os.getenv('CHLOE_USERNAME')\n",
    "password = os.getenv('CHLOE_PASSWORD')\n",
    "\n",
    "# Get the base directory where the HTML is located\n",
    "base_dir = os.path.dirname(os.path.abspath(chloe_local_path))\n",
    "\n",
    "def download_video(row):\n",
    "    if row['category'] != 'video' or pd.isna(row['video_stream_url']):\n",
    "        return\n",
    "\n",
    "    # Extract the ID from the link\n",
    "    parsed_url = urlparse(row['link'])\n",
    "    video_id = parsed_url.path.split('/')[-1]\n",
    "\n",
    "    # Clean the title (remove special characters and spaces)\n",
    "    clean_title = re.sub(r'[^\\w\\-_\\. ]', '', row['title'])\n",
    "    clean_title = clean_title.replace(' ', '_')\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"{row['date']}-{video_id}-{clean_title}.mp4\"\n",
    "\n",
    "    # Create full path for the file\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"File already exists: {full_path}\")\n",
    "        return\n",
    "\n",
    "    # Download the video\n",
    "    response = requests.get(row['video_stream_url'], auth=(username, password), stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(full_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Downloaded: {full_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {full_path}\")\n",
    "\n",
    "# Apply the download function to each row\n",
    "# Download all video files that don't exist yet\n",
    "for _, row in chloe_df[chloe_df['category'] == 'video'].iterrows():\n",
    "    download_video(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_poster(row):\n",
    "    # Extract the date and ID\n",
    "    date = row['date']\n",
    "    parsed_url = urlparse(row['link'])\n",
    "    video_id = parsed_url.path.split('/')[-1]\n",
    "\n",
    "    # Construct the poster URL\n",
    "    poster_url = f\"https://chloemorgane.com/posters/{date}-lg.jpg\"\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"{date}-{video_id}.jpg\"\n",
    "\n",
    "    # Create full path for the file\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"Poster already exists: {full_path}\")\n",
    "        return\n",
    "\n",
    "    # Download the poster\n",
    "    response = requests.get(poster_url, auth=(username, password))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(full_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"Downloaded poster: {full_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download poster: {full_path}\")\n",
    "\n",
    "# Apply the download function to each row\n",
    "for _, row in chloe_df.iterrows():\n",
    "    download_poster(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_html(row):\n",
    "    # Extract the date and ID\n",
    "    date = row['date']\n",
    "    parsed_url = urlparse(row['link'])\n",
    "    video_id = parsed_url.path.split('/')[-1]\n",
    "\n",
    "    # Construct the filename\n",
    "    filename = f\"{date}-{video_id}.html\"\n",
    "\n",
    "    # Create full path for the file\n",
    "    full_path = os.path.join(base_dir, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(full_path):\n",
    "        print(f\"HTML file already exists: {full_path}\")\n",
    "        return\n",
    "\n",
    "    # Download the HTML content\n",
    "    response = requests.get(row['link'], auth=(username, password))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(full_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(response.text)\n",
    "        print(f\"Downloaded HTML: {full_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download HTML: {full_path}\")\n",
    "\n",
    "# Apply the download function to each row\n",
    "for _, row in chloe_df.iterrows():\n",
    "    download_html(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def download_full_res_images(row):\n",
    "    # Extract the date and ID\n",
    "    date = row['date']\n",
    "    parsed_url = urlparse(row['link'])\n",
    "    video_id = parsed_url.path.split('/')[-1]\n",
    "\n",
    "    # Construct the HTML filename\n",
    "    html_filename = f\"{date}-{video_id}.html\"\n",
    "    html_full_path = os.path.join(base_dir, html_filename)\n",
    "\n",
    "    # Check if the HTML file exists\n",
    "    if not os.path.exists(html_full_path):\n",
    "        print(f\"HTML file not found: {html_full_path}\")\n",
    "        return\n",
    "\n",
    "    # Read the HTML content\n",
    "    with open(html_full_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all photo items\n",
    "    photo_items = soup.find_all('article', class_='photos-item')\n",
    "\n",
    "    for index, item in enumerate(photo_items, start=1):\n",
    "        # Find the link to the full resolution image\n",
    "        link = item.find('a')\n",
    "        if link and 'href' in link.attrs:\n",
    "            full_res_url = urljoin(row['link'], link['href'])\n",
    "            \n",
    "            # Construct the filename for the full resolution image\n",
    "            image_filename = f\"{date}-{video_id}-{index:03d}.jpg\"\n",
    "            image_full_path = os.path.join(base_dir, image_filename)\n",
    "\n",
    "            # Check if the image already exists\n",
    "            if os.path.exists(image_full_path):\n",
    "                print(f\"Image already exists: {image_full_path}\")\n",
    "                continue\n",
    "\n",
    "            # Download the full resolution image\n",
    "            response = requests.get(full_res_url, auth=(username, password))\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(image_full_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                print(f\"Downloaded full resolution image: {image_full_path}\")\n",
    "            else:\n",
    "                print(f\"Failed to download full resolution image: {full_res_url}\")\n",
    "\n",
    "# Apply the download function to each row of category 'photos'\n",
    "for _, row in chloe_df[chloe_df['category'] == 'photos'].sort_values('date').iterrows():\n",
    "    download_full_res_images(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(html_full_path):\n",
    "    # Read the HTML content\n",
    "    with open(html_full_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Try to find description in the first structure\n",
    "    card_div = soup.find('div', class_='card')\n",
    "    if card_div:\n",
    "        p_elements = card_div.find_all('p')\n",
    "        if p_elements:\n",
    "            return ' '.join(p.text.strip() for p in p_elements)\n",
    "\n",
    "    # If not found, try the second structure\n",
    "    sample_section = soup.find('section', class_='sample')\n",
    "    if sample_section:\n",
    "        p_elements = sample_section.find_all('p')\n",
    "        if p_elements:\n",
    "            return ' '.join(p.text.strip() for p in p_elements)\n",
    "\n",
    "    # If still not found, return None\n",
    "    return None\n",
    "\n",
    "# Function to get the HTML file path\n",
    "def get_html_file_path(row):\n",
    "    date = row['date']\n",
    "    parsed_url = urlparse(row['link'])\n",
    "    video_id = parsed_url.path.split('/')[-1]\n",
    "    base_dir = 'F:\\\\Culture Data Scraping\\\\Chloe Morgane'\n",
    "    html_filename = f\"{date}-{video_id}.html\"\n",
    "    return os.path.join(base_dir, html_filename)\n",
    "\n",
    "# Apply the function to create the new 'description' column\n",
    "chloe_df['description'] = chloe_df.apply(lambda row: extract_description(get_html_file_path(row)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "import pyarrow as pa\n",
    "\n",
    "# Convert Pandas DataFrame to PyArrow Table\n",
    "arrow_table = pa.Table.from_pandas(df)\n",
    "\n",
    "# Save as Delta table to a local directory\n",
    "local_path = \"F:\\\\Culture Data Scraping\\\\Chloe Morgane\\\\delta_table\"\n",
    "\n",
    "# Write the data to the Delta table\n",
    "# Use mode=\"overwrite\" to replace all data, or mode=\"append\" to add new data\n",
    "write_deltalake(local_path, arrow_table, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dotenv\n",
    "import os\n",
    "from libraries.client_stashapp import get_stashapp_client\n",
    "from libraries.StashDbClient import StashDbClient\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "stash = get_stashapp_client()\n",
    "\n",
    "stashbox_client = StashDbClient(\n",
    "    os.getenv(\"STASHDB_ENDPOINT\"),\n",
    "    os.getenv(\"STASHDB_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to generate the video filename based on the row data\n",
    "def get_video_filename(row):\n",
    "    if row['category'] == 'video':\n",
    "        date = row['date']\n",
    "        parsed_url = urlparse(row['link'])\n",
    "        video_id = parsed_url.path.split('/')[-1]\n",
    "        title = re.sub(r'[^\\w\\-_\\. ]', '', row['title'])  # Remove special characters\n",
    "        title = title.replace(' ', '_')  # Replace spaces with underscores\n",
    "        return f\"{date}-{video_id}-{title}.mp4\"\n",
    "    return None\n",
    "\n",
    "# Base directory for video files\n",
    "video_base_dir = 'F:\\\\Culture Data Scraping\\\\Chloe Morgane'\n",
    "\n",
    "# Create the new column 'video_filename'\n",
    "chloe_df['video_filename'] = chloe_df.apply(get_video_filename, axis=1)\n",
    "\n",
    "# Create the full path column\n",
    "chloe_df['video_filepath'] = chloe_df['video_filename'].apply(lambda x: os.path.join(video_base_dir, x) if x else None)\n",
    "\n",
    "# Check which files actually exist\n",
    "chloe_df['video_exists'] = chloe_df['video_filepath'].apply(lambda x: os.path.isfile(x) if x else None)\n",
    "\n",
    "# Print summary\n",
    "video_rows = chloe_df[chloe_df['category'] == 'video']\n",
    "print(f\"Total videos: {len(video_rows)}\")\n",
    "print(f\"Existing videos: {video_rows['video_exists'].sum()}\")\n",
    "print(f\"Missing videos: {len(video_rows) - video_rows['video_exists'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in chloe_df.iterrows():\n",
    "    if row['category'] == 'video':\n",
    "        scenes = stash.find_scenes({ \"path\": { \"modifier\": \"INCLUDES\", \"value\": row['video_filename'] } })\n",
    "        if len(scenes) == 0:\n",
    "            print(f\"Missing video: {row['video_filepath']}\")\n",
    "        if len(scenes) > 1:\n",
    "            print(f\"Duplicate video: {row['video_filepath']}\")\n",
    "        if len(scenes) == 1:\n",
    "            scene = scenes[0]\n",
    "            print(f\"Scene ID: {scene['id']}\")\n",
    "            print(f\"Scene Filename: {scene['title']}\")\n",
    "            print(f\"Scene Date: {scene['date']}\")\n",
    "            print(f\"Scene Rating: {scene['rating100']}\")\n",
    "            print(f\"Scene Tags: {scene['tags']}\")\n",
    "            print(f\"Scene Galleries: {scene['galleries']}\")\n",
    "            chloe_df.at[_, 'scene_id'] = scene['id']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in chloe_df.iterrows():\n",
    "    if row['category'] == 'video':\n",
    "        if row['scene_id']:\n",
    "            video_id = row['link'].split('/')[-1]\n",
    "            stash.update_scene({\n",
    "                \"id\": row['scene_id'],\n",
    "                \"code\": video_id\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "# Define the directory path\n",
    "directory = r\"F:\\Culture Data Scraping\\Chloe Morgane\"\n",
    "\n",
    "for _, row in chloe_df.iterrows():\n",
    "    if row['category'] == 'video' and row['scene_id']:\n",
    "        # Generate the filename using the same logic as in the example\n",
    "        # Parse the ID from the link\n",
    "        video_id = row['link'].split('/')[-1]\n",
    "        filename = f\"{row['date']}-{video_id}.jpg\"\n",
    "\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            # Read the image file and convert to base64\n",
    "            with open(file_path, \"rb\") as image_file:\n",
    "                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "            # Create the data URL\n",
    "            data_url = f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "\n",
    "            # Update the scene with the base64 data URL as cover_image\n",
    "            stash.update_scene({\n",
    "                \"id\": row['scene_id'],\n",
    "                \"cover_image\": data_url\n",
    "            })\n",
    "            print(f\"Updated cover image for scene: {row['title']}\")\n",
    "        else:\n",
    "            print(f\"Cover image not found for scene: {row['title']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chloe_morgane_scenes = stash.find_scenes({ \"studios\": { \"value\": [\"106\"], \"modifier\": \"INCLUDES\" } })\n",
    "df_chloe_morgane_scenes = pd.DataFrame(chloe_morgane_scenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store scenes by date\n",
    "scenes_by_date = {}\n",
    "\n",
    "# Populate the dictionary\n",
    "for scene in chloe_morgane_scenes:\n",
    "    date = scene.get('date')\n",
    "    if date:\n",
    "        if date not in scenes_by_date:\n",
    "            scenes_by_date[date] = []\n",
    "        scenes_by_date[date].append(scene)\n",
    "\n",
    "# Sort the dictionary by date\n",
    "sorted_scenes_by_date = dict(sorted(scenes_by_date.items()))\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = {date: scenes for date, scenes in sorted_scenes_by_date.items() if len(scenes) > 1}\n",
    "\n",
    "# Print the results\n",
    "if duplicates:\n",
    "    print(\"Duplicate scenes found:\")\n",
    "    for date, scenes in duplicates.items():\n",
    "        print(f\"\\nDate: {date}\")\n",
    "        for scene in scenes:\n",
    "            print(f\"  - Title: {scene.get('title', 'N/A')}\")\n",
    "            print(f\"    ID: {scene.get('id', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No duplicate scenes found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "name_of_my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
